# The Anatomy of a Search Engine

The repository contains a gradle applications project template for completing Assignment number 2.

### Project Structure

```bash
src
└── main
│    ├── java
          ├── com.example.searchengine
│    │              ├── Crawler.java # Abstract class for all crawlers. This class needs to be completed for task 1.task 2.
│    │              └── MultithreadCrawler.java #Instantiation of a multithreaded crawler. This class needs to be completed for task 4.
│    │              └── SearchEngine.java #Implementation of a search engine. This class needs to be completed for task 3.
                    └── SearchEngineApplication.java #Main Spring class, this class does not need to be modified.
                    └── SearchEngineProperties.java #Spring class to define the properties, which are read from the application.properties file. This file does not need to be modified.
│    │              └── SimpleCrawler.java #Instantiation of a simple crawler (with only one thread). This class needs to be completed for task 1.
│    ├──  resources
          ├── static
          │      └── index.html  # The main page of the search engine (TO COMPLETE)
│         └── index.csv  # CSV file to store the index generated by the crawler.
│         └── index_flipped.csv  # CSV file to store the flipped index generated by the index flipper.
│         └── search_engine_api.yaml  # OpenAPI specification of the Search Engine (TO COMPLETE)
└── test
      ├── TestBase.java # The base class for all tests. This class is not to be used independently.
      ├──CrawlerTest.java # The base class for all crawler tests. This class is not to be used independently.
      ├── SimpleCrawlerTest.java # The tests in this class needs to work to validate task 1.
      ├── MultithreadCrawlerTest.java # The tests in this class needs to work to validate task 4.
      ├── IndexFlipperTest.java # The tests in this class needs to work to validate task 2.
      ├── SearcherTest.java # The tests in this class needs to work to validate task 2.
      └── SearchEngineTest.java #T he test in this class needs to work to validate task 3.

├──.gitignore # A file used by git to indicate the files that are not added to git.
├── build.gradle # A file used by Gradle to indicate how to build the project.
├── gradlew # A file used by Gradle on Linux/Mac computers.
├── gradlew.bat # A file used by Gradle on Windows computers.
├── README.md # This file, with the instructions.
├── Report.md # The Report, to complete and send back
└── settings.gradle # A file used by Gradle
```

## Task 1


In this task, you have to implement a crawler that generates a CSV file [`index.csv`](src/main/resources/index.csv) that represents
the hypermedia environment. The crawling operation should start from the Web page at URL: https://api.interactions.ics.unisg.ch/hypermedia-environment/cc2247b79ac48af0.
The CSV file is a table that contains, for each Web page, the contents (three terms) that occur on that Web page.
You should first complete the `getInfo` method of the [`Crawler`](src/main/java/com/example/searchengine/Crawler.java) Crawler class (that will be reused by the MultithreadCrawler in task 4)
and then the [`SimpleCrawler`](src/main/java/com/example/searchengine/SimpleCrawler.java) class.

### Run the tests for the class SimpleCrawlerTest. You should have a successful build to pass the test.

> ⚠️ Tests are only indicative. If a test fails, your code is probably not correct but the test may pass while the code is not correct. This is true for all tests indicated in this README.

On Linux and macOS:

```bash
./gradlew test --tests "SimpleCrawlerTest"

```

On Windows:

```bash
.\gradlew test --tests "SimpleCrawlerTest"
```



## Task 2

In this task, you will implement another main component of your search engine, and experiment with it. For this, first, you have to complete the class [`IndexFlipper`](src/main/java/com/example/searchengine/IndexFlipper.java). This code should read the CSV file
[`index.csv`](src/main/resources/index.csv) from the crawler and invert the index. The inverted index should be in a file [`index_flipped.csv`](src/main/resources/index_flipped.csv).

Then, you should complete the `search` method of the [`Searcher`](src/main/java/com/example/searchengine/Searcher.java) class to be able to retrieve all URLs containing a keyword in the [`index_flipped.csv`](src/main/resources/index_flipped.csv) file.

### Run the tests for the class IndexFlipperTest, and then SearcherTest. You should have a sucessful build to pass the test.

These tests should be performed only after the index.csv file has been created. You should perform the tests for the IndexFlipper before performing the tests for the Searcher in order to have the index_flipped.csv file properly initialized.

On Linux and macOS:

```bash
./gradlew test --tests "IndexFlipperTest"
./gradlew test --tests "SearcherTest"
```




On Windows:

```bash
.\gradlew test --tests "IndexFlipperTest"
.\gradlew test --tests "SearcherTest"
```




## Task 3

The search engine is available as a Web service, which you will implement as part of this task. In
order to do so, you have to complete the program SearchEngine.java. This task has three parts.
In the first part, you will implement an HTTP API for your search engine based on a provided
OpenAPI specification. In the second part, you will create a user interface for the search engine.
In the third part, you will extend, but not implement, the provided OpenAPI specification with an
administrator interface.

### Search Engine API

We provide you with an OpenAPI specification [`search_engine_api.yaml`](src/main/resources/search_engine_api.yaml)
for your search engine’s API in the assignment package. Your task is to complete the [`SearchEngine`](src/main/java/com/example/searchengine/SearchEngine.java) class to create a server that implements the API.
The search engine API supports two operations: a “search” operation that allows users to get
the URLs of the pages containing a certain keyword in a JSON array, and a “lucky” operation
performs a redirection to a Web page in the environment containing the keyword if one exists.

You should first configure the file application.properties in [`src/main/resources`](src/main/resources):

- `server.port` defines the port. You should not need to change the port 8080.
- `crawler` indicates the crawler to be used ("simple" for the SimpleCrawler. You can also use "multithread" for the MultithreadCrawler, once it has been implemented in Task 4).
- `crawl` is a boolean ("true" or "false") indicating whether the search engine should crawl the environment when the application is starting. 
If the file index.csv and index_flipped.csv are empty, then you should set crawl=true. Else, you can set crawl=false and the search engine will use the existing index_flipped.csv file.

Then, you can run the search engine.

On Linux and macOS:

```bash
./gradlew bootRun

```


On Windows:

```bash
.\gradlew bootRun

```

### Run the tests for the class SearchEngine. You should have a sucessful build to pass the test.

On Linux and macOS:

```bash
./gradlew test --tests "SearchEngineTest"

```


On Windows:

```bash
.\gradlew test --tests "SearchEngineTest"
```

When the search engine is running, go to the URL: "http://localhost:8080/" (or update the port if needed).
At first, you should see a blank page. This is normal.


### User Interface

When opening your browser to the URL of the server (e.g., http://localhost:8080), you will see an HTML page with a body that is not visible but with the title set to "Search Engine". What you can see is the [`index.html`](src/main/resources/static/index.html) page provided in the
template. You should update this HTML page so that it has a text field to enter a search term
and two buttons. When clicking the first button, Search, the user interface should render all URLs that this search term occurs on as clickable hyperlinks. This operation is done by the getUrls Javascript function that you have to complete. To do so, first update the URL to call with the PORT that you use. Then, perform an HTTP request with the [`Fetch API`](https://developer.mozilla.org/en-US/docs/Web/API/Fetch_API) to the /search API in order to get the URLs containing the keyword. Finally,add these URLs to the list with id="urls". Then, When clicking the second button, I’m feeling Lucky, the user interface should directly take the user to any of the pages that certainly contain the specific term. 

You should:
- add an input field to write the keyword.
- Create a Search Button that relies on a Javascript function getUrls to complete by calling the search API with the Javascript Fetch API or Axios and then by updating the Web page through DOM manipulation.
- Create an "I'm feeling Lucky" button.

### Extending the API specification

Your task is to extend the OpenAPI specification in the file [`search_engine_api.yaml`](src/main/resources/search_engine_api.yaml), in [`src/main/resources`](src/main/resources), by writing the administration interface. The administration interface should enable administrators to launch a new crawling operation, to regenerate the flipped index, to delete a URL from the index, and update (or add) the keywords associated with a given URL
in the index. You are also required to document all your design decisions as part of this task in Report.md. However, you are not required to implement these features in your search engine. Your design decisions should respect the semantics of the HTTP methods and status codes. You can use the [`Pet Store OpenAPI Specification`](https://petstore.swagger.io/#/) as inspiration.





## Task 4

Since the crawler spends a lot of time waiting for responses from the Web server in general,  multi-threading would lead to a large performance increase for your crawler.

Complete the code of the class [`MultithreadCrawler`](src/main/java/com/example/searchengine/MultithreadCrawler.java) to implement a multithread crawler. You can do so by completing the `crawl` method and the `CrawlerRunnable` and `ObserveRunnable` inner classes.

Indicate in the Report.md the time necessary for the SimpleCrawler (that you created in Task 1) to work, the time necessary for the MultithreadedCrawler to work, and indicate the ratio as SimpleCrawler / MultithreadedCrawler.

### Run the tests for the class MultithreadCrawlerTest. You should have a sucessful build to pass the test.

On Linux and macOS:

```bash
./gradlew test --tests "MultithreadCrawlerTest"

```


On Windows:

```bash
.\gradlew test --tests "MultithreadCrawlerTest"
```

